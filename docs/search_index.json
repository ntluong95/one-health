[["index.html", "Applied Epidemiology in One Health Research Chapter 1 Basic Epidemiology", " Applied Epidemiology in One Health Research Author: Nguyn Thanh Lng Date updated: 2021-01-31 Chapter 1 Basic Epidemiology OBJECTIVES After reading this chapter, reader should be able to: Construct a logical causal diagram based on your area of research interest as an aid to guiding your study design and analyses Select the appropriate sampling strategy for a particular situation, taking into account the requirements, advantages and disadvantages of each method. Identify the different types of selection bias and assess whether or not a particular study is likely to suffer from excess selection bias. Explain the different ways of measuring disease frequency and differentiate among counts, proportions, odds and rates. Calculate and interpret the following measures of association: risk ratio, odds ratio, incidence rate ratio, risk difference (attributable risk), attributable fraction (exposed), population attributable risk, attributable fraction (population). Understand the strength and weaknesses of various study designs for the identification and evaluation of causal factors. "],["about-the-book.html", "1.1 About the book", " 1.1 About the book Our rationale for doing research is to identify potentially causal associations between exposures and outcomes (the center of the diagram). In many cases, the exposures are risk factors and the outcome is a disease of interest. However, this is not the only scenario; for example, our outcome of interest might be a measure of productivity or food safety and the exposures might include certain diseases. Figure 1.1. Key components of epidemiologic research Ultimately, we aim to make causal inferences (bottom right of diagram) and Chapter 1 discusses some important concepts of causation as they relate to epidemiologic research. Any study starts with an overall study design and the main observational study types are discussed in Chapters 1.6. In any study, it is important to identify the target population and obtain a study group from it in a manner that does not lead to selection bias. Sampling and selection bias is discussed in Chapter 1.3. Once we have identified our study subjects, it is necessary to obtain data on exposure variables, extraneous variables and the outcome in a manner that does not lead to information bias (Chapter 1.7). Two important tools that are used in that process are questionnaires (Chapter 1.4) and diagnostic and screening tests (Chapter 1.4). In order to start the process of establishing an association between exposure and outcome, we need to settle on a measure of disease frequency (Chapter 1.3) and select a measure of association (Chapter 1.5) that fits the context. In many cases, the study design will determi4e the measures that are appropriate. Confounding bias is a major concern in observational studies, and the identification of factors that should be controlled as confounders is featured in Chapter 1.7. Basic data analysis will be presented in Chapter 2. You will learn essential skills to conduct data analysis with different programming languages such as R and Python or statistical software like STATA. You will also introduced on how to communicate your works with other scientists and readers using Rmarkdown (Chapter 2.7) and ShinyApp (Chapter 2.8). Chapter 2.9 will provides you a road map for investigators starting into the analysis of an epidemiologic dataset. With our data in hand, we are now able to begin to model relationships with the intent of estimating causal effects of exposure (Chapter 3). Individual chapters are dedicated to the analyses appropriate for outcomes that are continuous (Chapter 3.2), dichotomous (Chapter 3.3), nominal/ordinal (Chapter 3.4), count (Chapter 3.5) and time-to-event data (Chapter 3.6). Chapter 3.1 presents some general guide lines on model-building techniques that are applicable to all types of model. In one health epidemiologic research, we often encounter clustered or correlated data and these present major challenges in their analyses. Chapter 3.7.1 introduces these while Chapters 3.7.2 and Chapters 3.7.3 focus on mixed (random effects) models for continuous and discrete outcomes. Chapters 3.7.4 presents some alternative methods of analysis for dealing with clustered data. Other important tools in epidemiology, including Risk analysis (Chapter 4), Spatial epidemiology (Chapter 5), Estimating true prevalence of a diseases (Chapter 6) and Infections diseases modelling (Chapter 7) will also be discussed Other models and tools will be mentioned in Chapter 8. Structured reviews and assessments of the literature in the form of meta-analyses are becoming increasingly important and are introduced in Chapter 8.3. In Chapter 9, we will introduce about digital data collection tool (Chapter 9.1) and visualization tools (Chapter 9.2) "],["causual-concept.html", "1.2 Causual concept", " 1.2 Causual concept Chapter 1 "],["sampling.html", "1.3 Sampling", " 1.3 Sampling Chapter 2 v√† Chapter 12 "],["measures-of-diseases-frequency.html", "1.4 Measures of diseases frequency", " 1.4 Measures of diseases frequency Chapter 4 "],["measures-of-association.html", "1.5 Measures of association", " 1.5 Measures of association Chapter 6 "],["introduction-about-different-studies-design.html", "1.6 Introduction about different studies design", " 1.6 Introduction about different studies design 1.6.1 Introduction about observational studies Chapter 7 1.6.2 Cohort studies Chapter 8 1.6.3 Case-control studies Chapter 9 1.6.4 Hybrid study designs Chapter 10 1.6.5 Longitudinal study and time series data To be searched 1.6.6 Controlled trials Chapter 11 "],["confounder-bias.html", "1.7 Confounder bias", " 1.7 Confounder bias Chapter 13 "],["basic-data-analysis.html", "Chapter 2 Basic Data analysis", " Chapter 2 Basic Data analysis OBJECTIVES After reading this chapter, reader should be able to: Construct a logical causal diagram based on your area of research interest as an aid to guiding your study design and analyses Select the appropriate sampling strategy for a particular situation, taking into account the requirements, advantages and disadvantages of each method. Identify the different types of selection bias and assess whether or not a particular study is likely to suffer from excess selection bias. Explain the different ways of measuring disease frequency and differentiate among counts, proportions, odds and rates. Calculate and interpret the following measures of association: risk ratio, odds ratio, incidence rate ratio, risk difference (attributable risk), attributable fraction (exposed), population attributable risk, attributable fraction (population). Understand the strength and weaknesses of various study designs for the identification and evaluation of causal factors. "],["introduction-about-work-flow.html", "2.1 Introduction about work flow", " 2.1 Introduction about work flow The picture below described about a normal working process of data analysis. First you must import your data. This typically means that you take data stored in a file, database, or web application programming interface (API), and load it into the software. In this course, we will work with .xlsx and .csv file, which are the most common type of storaging data. Source: R for Data Science book The next steps are Tidying and Transforming your data. Combining 2 steps, we have the so-called Data Wrangling. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Once you have tidy data, a common first step is to transform it. Transformation includes narrowing in on observations of interest, creating new variables that are functions of existing variables, and calculating a set of summary statistics (like counts or means). Visualization is a fundamentally human activity. A good visualization will show you things that you did not expect, or raise new questions about the data. A good visualization might also hint that youre asking the wrong question, or you need to collect different data. Visualizations can surprise you, but dont scale particularly well because they require a human to interpret them. Models are complementary tools to visualization. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you. The last step of data science is Communication, an absolutely critical part of any data analysis project. It doesnt matter how well your models and visualization have led you to understand the data unless you can also communicate your results to others. "],["software-installation.html", "2.2 Software installation", " 2.2 Software installation In this course, I will introduce you about the three most powerful statistical softwares including R, Python and STATA, their usage and their advantages as well as disadvantages. While R and Python is more a programming language, which can be a hurdle for some people, STATA is more user-friendly and balance between using Graphical User Interface (GUI) and saving script (do file). In short, youll need to install the following programs in order: R: A statistical programming language used to wrangle, analyze, and visualize data (mac, windows) RStudio: An interface for writing and running R code, which is a primary language for the quarter (link). You can download a developing version here, which will provide most up-to-date new features Python: Another programming language, be preferred in machine learning, deep learning, AI. In this tutorial, we will install Python indirectly through package reticulate in R. More details will be presented later Git: (Optional) A set of command-line tools for tracking changes to a project. This is likely already installed on Macs. The Windows download will come with Git Bash, a simple interface for executing Git commands (link) STATA: This is paid software. You have to buy it before use or you can download a trial version here The following sections have additional information about the purpose of each component, how to install it, and alternative configurations. 2.2.1 R R is a popular data science language used to download, analyze, and visualize data. You can download it at the appropriate link for your operating system (mac, windows). At the link, click the appropriate download link and follow instructions: To install package in R, type the following code to the console. For instance, you want to install tidyverse package install.packages(&quot;tidyverse&quot;) To call tidyverse package library(tidyverse) 2.2.2 Rstudio The primary programming language we will use throughout the course is R. Its a very powerful statistical programming language that is built to work well with large and diverse datasets. While you are able to execute R scripts without an interface, the RStudio interface provides a wonderful way to engage with the R language. Importantly, you cannot use the RStudio interface until you have installed R. To download the RStudio program, select the installer for your operating system at this link. Make sure to scroll down to download a free version of RStudio: 2.2.3 Python Python is a very popular all-purpose programming language that is making a major impact in the data-science arena. Some data scientists, and even some organizations, believe they have to pick between R or Python. However, this turns out to be a false choice. In fact, results from a surveys showed that many data science teams today are bilingual, leveraging both R and Python in their work. And while both languages have unique strengths, these teams frequently struggle to use them together. Rstudio provides a prefect environment for data scientist through a package reticulate. Rstudio can be a potentially platform for data scientist who are using solely R or Python, or using both. 2.2.4 Setting environment for R and Python Firstly, you have to install reticulate package: install.packages(&quot;reticulate&quot;) Secondly, install the miniconda environment. I suggest you should install miniconda in your local disk, not your OS disk (i.e Disk C) reticulate::install_miniconda(&quot;D:/Python&quot;) reticulate::use_condaenv(&quot;D:/Python/envs/r-reticulate&quot;) Thirdly, try to install Python package. There are two ways to do it, Reticulate style and Python style Reticulate style library(reticulate) py_install(&quot;pandas&quot;) py_install(&quot;matplotlib&quot;) Python style Set Python PATH through Terminal. You copy this code and paste into Terminal console in your Rstudio: setx PATH \"%PATH%;D:\\Python\\envs\\r-reticulate\\Scripts\" Then try to run this code to install pandas package: python -m pip install pandas 2.2.5 STATA This is paid software. You have to buy it before use or you can download a trial version here If you want to use STATA in Rstudio, you can do it through statamarkdown package. Detail will be dicussed further in this document. install.packages(&quot;Statamarkdown&quot;) library(Statamarkdown) stataexe &lt;- &quot;C:/Program Files/Stata16/StataSE-64.exe&quot; knitr::opts_chunk$set(engine.path=list(stata=stataexe)) "],["create-working-environment.html", "2.3 Create working environment", " 2.3 Create working environment Git is a version control system that provides a set of commands that allow you to manage changes to a project (much more on this in module-3). For now, youll need to download and install the software. Note, if you are using a Windows machine, this will install a program called Git Bash, which provides a text-based interface for executing commands on your computer. To setup Git in your Rstudio, you can visit this link. For alternative/additional Windows command-line tools, see below: Git Bash Because well primarily use the command line for implementing version control (i.e., keeping track of changes to our code), we can use a command-line tool that ships with the version control software, Git. When you download the Git software on Windows, the Git Bash user-interface will be installed. You can then navigate to Git Bash from your Desktop / Start Menu, and you will be able to use the appropriate syntax to keep track of code changes. Windows Bash With the release of Windows 10, Windows began providing command line (bash) support. If you already have Windows 10, here are a few instructions for installing bash capabilities. This requires that you switch to 64 bit windows, and follow the instructions above. While this will provide you with direct bash capabilities, you may run into challenges along the way (I have not tested these instructions). Note, you will still need to install Git in addition to Windows Bash. Powershell (Windows Management Framework) If you want to explore more robust command-line alternatives for Windows, the Windows Management Framework (including a program called Powershell) seems to be a preferred standard. Powershell will provide a simple text-based interface for inputing commands. Note, you will still need to install Git in addition to Powershell. "],["data-manipulation.html", "2.4 Data manipulation", " 2.4 Data manipulation 2.4.1 Import file There are several ways to import data, but the most preferred is from comma-separated values file (.csv). Other data types format you may normally see in your work are from text file (.txt), Excel spread sheets (.xlsx), STATA file (.dta), SPSS file (.sav), JavaScript Object Notation (.json), shapefile (.shp). In this tutorial, I will only concentrate on importing data from CSV file because of its ubiquitous in data analysis. We will use calf data set, you can download it here. Dataset description These data come from a retrospective analysis of the medical records from all diarrheic calves which were presented to Atlantic Veterinary College, PEl, Canada between 1989 and 1993. There are 254 observations (records) and 14 variables in the dataset (calf): Variables Description Codes/Units case hospital case number age age at admission days breed breed coded 1-9 sex sex 0 = female 1 = male attd attitude of calf 0 = bright, alert 1 = depressed 2 = unresponsive, comatose dehy % dehydration eye uveitis/hypopyon clinically evident 0/1 jnts swollen joints clinically evident number of joints affected post posture of caIf 0 = standing 1 = sternal 2 = lateral pulse pulse rate beats per minute resp respiratory rate breaths per minute temp rectal temperature 0C umb swollen umbilicus clinically evident 0/1 sepsis sepsis (outcome) 0/1 R To import data in R, you need readr package installed in your computer and use function read_csv to import file to the working environment. library(readr) raw_df &lt;- read_csv(&quot;D:/Book Writing/1. Applied Epidemiology in One Health/data/calf.csv&quot;) glimpse(raw_df) ## Rows: 254 ## Columns: 14 ## $ case &lt;dbl&gt; 1670, 8124, 6954, 2737, 5341, 6749, 3234, 2325, 2925, 7108, ... ## $ age &lt;dbl&gt; 5, 3, 2, 3, 3, 10, 4, 14, 4, 8, 19, 7, 6, 2, 18, 7, 7, 8, 12... ## $ breed &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ## $ sex &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, ... ## $ attd &lt;dbl&gt; 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 2, 0, 1, ... ## $ dehy &lt;dbl&gt; 12.0, 13.5, NA, 5.0, 0.0, 5.5, 7.0, 7.0, 5.0, 10.0, 8.0, 4.0... ## $ eye &lt;dbl&gt; NA, 0, 1, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ jnts &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ## $ post &lt;dbl&gt; 2, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 2, 0, 1, ... ## $ pulse &lt;dbl&gt; NA, 130, NA, 132, 128, 160, 180, 100, 150, 120, 140, 126, 10... ## $ resp &lt;dbl&gt; NA, 120, NA, 40, 48, 48, 84, 20, 52, 68, 32, 20, 20, 16, 40,... ## $ temp &lt;dbl&gt; 37.6, 39.2, NA, 38.6, 38.6, 39.6, 39.0, 35.0, 39.5, 37.8, 39... ## $ umb &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ... ## $ sepsis &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, ... Python First, you have to register to use Python in R via package reticulate reticulate::use_condaenv(&quot;D:/Python/envs/r-reticulate&quot;) import os as os os.environ[&#39;QT_QPA_PLATFORM_PLUGIN_PATH&#39;] = &quot;D:/Python/envs/r-reticulate/Library/plugins/platforms&quot; Now, you can import CSV file by Python using library pandas with function read_csv import pandas as pd raw_df = pd.read_csv(&quot;D:/Book Writing/1. Applied Epidemiology in One Health/data/calf.csv&quot;) raw_df.head() ## case age breed sex attd dehy ... post pulse resp temp umb sepsis ## 0 1670 5.0 1 1.0 2.0 12.0 ... 2.0 NaN NaN 37.6 0.0 0 ## 1 8124 3.0 2 0.0 1.0 13.5 ... 0.0 130.0 120.0 39.2 0.0 0 ## 2 6954 2.0 3 1.0 2.0 NaN ... 2.0 NaN NaN NaN 1.0 1 ## 3 2737 3.0 4 1.0 1.0 5.0 ... 0.0 132.0 40.0 38.6 0.0 0 ## 4 5341 3.0 5 0.0 1.0 0.0 ... 1.0 128.0 48.0 38.6 1.0 1 ## ## [5 rows x 14 columns] STATA First, setup STATA code chunk in Rmarkdown library(Statamarkdown) stataexe &lt;- &quot;C:/Program Files/Stata16/StataSE-64.exe&quot; knitr::opts_chunk$set(engine.path=list(stata=stataexe)) Then, use function import delimited, specify your path to the data import delimited &quot;D:\\Book Writing\\1. Applied Epidemiology in One Health\\data\\calf.csv&quot; describe . import delimited &quot;D:\\Book Writing\\1. Applied Epidemiology in One Health\\da&gt; calf.csv&quot; (14 vars, 254 obs) . describe Contains data obs: 254 vars: 14 ------------------------------------------------------------------------------- storage display value variable name type format label variable label ------------------------------------------------------------------------------- case int %8.0g age byte %8.0g breed byte %8.0g sex byte %8.0g attd byte %8.0g dehy float %9.0g eye byte %8.0g jnts byte %8.0g post byte %8.0g pulse int %8.0g resp int %8.0g temp float %9.0g umb byte %8.0g sepsis byte %8.0g ------------------------------------------------------------------------------- Sorted by: Note: Dataset has changed since last saved. 2.4.2 Labelling data 2.4.2.1 R To label for value of variables in dataset, you can use function mututate to create new varibales (will present in details later) combining with factor function. Remember that you can only do that for categorical variables, in R they is storaged as factor data. Noted that the operation |&gt; means forwarding. It is R native piping, similar to %&gt;% if you know magrittr package lb_df &lt;- raw_df |&gt; mutate(breed = factor(breed, level = c(1:9), labels = c(&quot;Hereford&quot;, &quot;Holstein&quot;,&quot;Angus&quot;, &quot;Shorthorn&quot;, &quot;Charolais&quot;, &quot;Simmental&quot;, &quot;Limousin&quot;, &quot;Crossbreed&quot;, &quot;Other&quot;)), sex = factor(sex, level = c(0:1), labels = c(&quot;Female&quot;,&quot;Male&quot;)), attd = factor(attd, level = c(0:2), labels = c(&quot;bright, alert&quot;,&quot;depressed&quot;, &quot;unresponsive, comatose&quot;)), eye = factor(eye, level = c(0:1), labels = c(&quot;No&quot;,&quot;Yes&quot;)), post = factor(post, level = c(0:2), labels = c(&quot;standing&quot;,&quot;sternal&quot;, &quot;lateral&quot;)), umb = factor(umb, level = c(0:1), labels = c(&quot;No&quot;,&quot;Yes&quot;)), sepsis = factor(sepsis, level = c(0:1), labels = c(&quot;No&quot;,&quot;Yes&quot;))) To label for variables in dataset, you can use function var_labels in package sjlabelled. library(magrittr) lb_df %&lt;&gt;% sjlabelled::var_labels( case = &quot;hospital case number&quot;, age = &quot;age at admission&quot;, breed = &quot;breed&quot;, sex = &quot;sex&quot;, attd = &quot;attitude of calf&quot;, dehy = &quot;% dehydration&quot;, eye = &quot;uveitis/hypopyon clinically evident&quot;, jnts = &quot;swollen joints clinically evident&quot;, post = &quot;posture of caIf&quot;, pulse = &quot;pulse rate&quot;, resp = &quot;respiratory rate&quot;, temp = &quot;rectal temperature&quot;, umb = &quot;swollen umbilicus clinically evident&quot;, sepsis = &quot;sepsis&quot;) You can view the result through function view(lb_df), it will open the window like this: 2.4.2.2 Python Python doesnt support to handle with labelled data, or complicated to do. One simple solution is we do it in R and then pass to Pandas data frame by calling function r. lb_df = r.lb_df If you still want to try, an alternative is creating a dictionary that contain all value of variables. Then using mutate function from siuba library which is similar to dplyr library in R Tidyverse to mutate new variable breed_dict = {1 : &quot;Hereford&quot;, 2 : &quot;Holstein&quot;, 3 : &quot;Angus&quot;, 4 : &quot;Shorthorn&quot;, \\ 5 : &quot;Charolais&quot;, 6: &quot;Simmental&quot;, 7 : &quot;Limousin&quot;, 8: &quot;Crossbreed&quot;, 9 : &quot;Other&quot;} sex_dict = {0 : &quot;Female&quot;, 1 : &quot;Male&quot;} attd_dict = {0 : &quot;bright, alert&quot;, 1: &quot;depressed&quot;, 2: &quot;unresponsive, comatose&quot;} eye_dict = {0 : &quot;No&quot;, 1 : &quot;Yes&quot;} post_dict = {0 : &quot;standing&quot;, 1: &quot;sternal&quot;, 2 : &quot;lateral&quot;} umb_dict = {0 : &quot;No&quot;, 1 : &quot;Yes&quot;} sepsis_dict = {0 : &quot;No&quot;, 1 : &quot;Yes&quot;} from siuba import * lb_df1 = raw_df &gt;&gt; mutate( sex = _.apply(lambda x: sex_dict.get(x[&quot;sex&quot;], &quot;NaN&quot;), axis = 1), breed = _.apply(lambda x: breed_dict.get(x[&quot;breed&quot;], &quot;NaN&quot;), axis = 1), attd = _.apply(lambda x: attd_dict.get(x[&quot;attd&quot;], &quot;NaN&quot;), axis = 1), eye = _.apply(lambda x: eye_dict.get(x[&quot;eye&quot;], &quot;NaN&quot;), axis = 1), post = _.apply(lambda x: post_dict.get(x[&quot;post&quot;], &quot;NaN&quot;), axis = 1), umb = _.apply(lambda x: umb_dict.get(x[&quot;umb&quot;], &quot;NaN&quot;), axis = 1), sepsis = _.apply(lambda x: sepsis_dict.get(x[&quot;sepsis&quot;], &quot;NaN&quot;), axis = 1)) You can do exactly same version with assign function from pandas library. The principle is same because siuba was developed inherited from pandas, but lighter and support for R users who are familiar with tidyverse ecosystem. lb_df2 = raw_df.assign( sex = _.apply(lambda x: sex_dict.get(x[&quot;sex&quot;], &quot;NaN&quot;), axis = 1), breed = _.apply(lambda x: breed_dict.get(x[&quot;breed&quot;], &quot;NaN&quot;), axis = 1), attd = _.apply(lambda x: attd_dict.get(x[&quot;attd&quot;], &quot;NaN&quot;), axis = 1), eye = _.apply(lambda x: eye_dict.get(x[&quot;eye&quot;], &quot;NaN&quot;), axis = 1), post = _.apply(lambda x: post_dict.get(x[&quot;post&quot;], &quot;NaN&quot;), axis = 1), umb = _.apply(lambda x: umb_dict.get(x[&quot;umb&quot;], &quot;NaN&quot;), axis = 1), sepsis = _.apply(lambda x: sepsis_dict.get(x[&quot;sepsis&quot;], &quot;NaN&quot;), axis = 1)) print(lb_df) case age breed sex ... resp temp umb sepsis 0 1670.0 5.0 Hereford Male ... NaN 37.6 No No 1 8124.0 3.0 Holstein Female ... 120.0 39.2 No No 2 6954.0 2.0 Angus Male ... NaN NaN Yes Yes 3 2737.0 3.0 Shorthorn Male ... 40.0 38.6 No No 4 5341.0 3.0 Charolais Female ... 48.0 38.6 Yes Yes .. ... ... ... ... ... ... ... ... ... 249 4870.0 1.0 Other Female ... 48.0 39.3 Yes Yes 250 9805.0 3.0 Other Female ... 28.0 39.0 No No 251 4869.0 2.0 Other Female ... 30.0 39.3 No No 252 2926.0 3.0 Other Female ... 70.0 39.8 No No 253 4186.0 25.0 Other Female ... NaN 39.7 NaN No [254 rows x 14 columns] 2.4.2.3 STATA label var case &quot;hospital case number&quot; label var age &quot;age at admission&quot; label var breed &quot;breed&quot; label def breed 1 &quot;Hereford&quot; 2 &quot;Holstein&quot; 3 &quot;Angus&quot; 4 &quot;Shorthorn&quot; 5 &quot;Charolais&quot; 6 &quot;Simmental&quot; 7 &quot;Limousin&quot; 8 &quot;Crossbreed&quot; 9 &quot;Other&quot; label val breed breed label var sex &quot;sex&quot; label def sex 0 &quot;Female&quot; 1 &quot;Male&quot; label val sex sex label var attd &quot;attitude of calf&quot; label def attd 0 &quot;bright, alert&quot; 1 &quot;depressed&quot; 2 &quot;unresponsive, comatose&quot; label val attd attd label var dehy &quot;% dehydration&quot; label var eye &quot;uveitis/hypopyon clinically evident&quot; label def eye 0 &quot;No&quot; 1 &quot;Yes&quot; label val eye eye label var jnts &quot;swollen joints clinically evident&quot; label var post &quot;posture of caIf&quot; label def post 0 &quot;standing&quot; 1 &quot;sternal&quot; 2 &quot;lateral&quot; label val post post label var pulse &quot;pulse rate&quot; label var resp &quot;respiratory rate&quot; label var temp &quot;rectal temperature&quot; label var umb &quot;swollen umbilicus clinically evident&quot; label def umb 0 &quot;No&quot; 1 &quot;Yes&quot; label val umb umb label var sepsis &quot;sepsis&quot; label def sepsis 0 &quot;No&quot; 1 &quot;Yes&quot; label val sepsis sepsis 2.4.3 Data transformation 2.4.3.1 R In this section you are going to learn the five key dplyr functions that allow you to solve the vast majority of your data manipulation challenges: Pick observations by their values filter(). Reorder the rows arrange(). Pick variables by their names select(). Create new variables with functions of existing variables mutate(). Collapse many values down to a single summary summarise(). These can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation. All verbs work similarly: The first argument is a data frame. The subsequent arguments describe what to do with the data frame, using the variable names (without quotes). The result is a new data frame. Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Lets dive in and see how these verbs work. Create new variable 2.4.3.2 Python 2.4.3.3 STATA "],["descriptive-statistics.html", "2.5 Descriptive statistics", " 2.5 Descriptive statistics "],["data-visualization.html", "2.6 Data visualization", " 2.6 Data visualization "],["communication-with-rmarkdown.html", "2.7 Communication with Rmarkdown", " 2.7 Communication with Rmarkdown R Markdown is an open-source tool for producing reproducible reports in R. It enables you to keep all of your code, results, plots, and writing in one place. R Markdown is particularly useful when you are producing a document for an audience that is interested in the results from your analysis, but not your code. In general, you can not only use Rmarkdown for your R code, but you can integrate many others, such as Python and STATA like what we are doing in this tutorial book. R Markdown is powerful because it can be used for data analysis and data science, collaborating with others, and communicating results to decision makers. With R Markdown, you have the option to export your work to numerous formats including PDF, Microsoft Word, a slideshow, or an HTML document for use in a website. You can find a comprehensive guide from the book: R Markdown: The Definitive Guide 2.7.1 Start a document with Rmarkdown R Markdown is a free, open source tool that is installed like any other R package. Use the following command to install R Markdown: install.packages(&quot;rmarkdown&quot;) Now that R Markdown is installed, open a new R Markdown file in RStudio by navigating to File &gt; New File &gt; R Markdown. R Markdown files have the file extension .Rmd. When you open a new R Markdown file in RStudio, a pop-up window appears that prompts you to select output format to use for the document. The default output format is HTML. With HTML, you can easily view it in a web browser. 2.7.2 Rmarkdown document format Once youve selected the desired output format, an R Markdown document appears in your RStudio pane. But unlike an R script which is blank, this .Rmd document includes some formatting that might seem strange at first. Lets break it down. Weve highlighted six different sections of this R Markdown document to understand what is going on: YAML Header: Controls certain output settings that apply to the entire document. Code Chunk: Includes code to run, and code-related options. Body Text: For communicating results and findings to the targeted audience. Code to Generate a Table: Outputs a table with minimal formatting like you would see in the console. Section Header: Specified with ##. Code to Generate a Plot: Outputs a plot. Here, the code used to generate the plot will not be included because the parameter echo=FALSE is specified. This is a chunk option. Well cover chunk options soon! This document is ready to output as-is. Lets knit, or output, the document to see how these formatting specifications look in a rendered document. We do this in RStudio by clicking the knit button. Knitting the document generates an HTML document, because thats the output format weve specified. The shortcut to knit a document is Command + Shift + K on a Mac, or Ctrl + Shift + K on Linux and Windows. The k is short for knit! If you want to share your work to the internet, Rstudio provides a wonderful platform called Rpubs. To do this, after you knit your document, you click on the button Publish document on the top right, create an account on Rpubs and wait to see your results. 2.7.3 Control Behavior with Code Chunk Options One of the great things about R Markdown is that you have many options to control how each chunk of code is evaluated and presented. This allows you to build presentations and reports from the ground up  including code, plots, tables, and images  while only presenting the essential information to the intended audience. For example, you can include a plot of your results without showing the code used to generate it. Mastering code chunk options is essential to becoming a proficient R Markdown user. The best way to learn chunk options is to try them as you need them in your reports, so dont worry about memorizing all of this now. Here are the key chunk options to learn: echo = FALSE: Do not show code in the output, but run code and produce all outputs, plots, warnings and messages. The code chunk to generate a plot in the image below is an example of this. eval = FALSE: Show code, but do not evaluate it. fig.show = hide: Hide plots. include = FALSE: Run code, but suppress all output. This is helpful for setup code. You can see an example of this in the top code chunk of the image below. message = FALSE: Prevent packages from printing messages when they load. This also suppress messages generated by functions. results = \"hide\": Hides printed output. warning = FALSE: Prevents packages and functions from displaying warnings. "],["shinyapp.html", "2.8 ShinyApp", " 2.8 ShinyApp "],["a-structred-approach-to-data-analysis.html", "2.9 A structred approach to data analysis", " 2.9 A structred approach to data analysis "],["regression-models.html", "Chapter 3 Regression models ", " Chapter 3 Regression models "],["model-buidling-strategy.html", "3.1 Model buidling strategy", " 3.1 Model buidling strategy "],["linear-regression.html", "3.2 Linear regression", " 3.2 Linear regression "],["logistic-regression.html", "3.3 Logistic regression", " 3.3 Logistic regression "],["modelling-multinomial-data.html", "3.4 Modelling multinomial data", " 3.4 Modelling multinomial data "],["modelling-count-and-rate-data.html", "3.5 Modelling count and rate data", " 3.5 Modelling count and rate data "],["modelling-survival-data.html", "3.6 Modelling survival data", " 3.6 Modelling survival data "],["modelling-clustered-data.html", "3.7 Modelling clustered data", " 3.7 Modelling clustered data 3.7.1 Introduction to clustered data 3.7.2 Mixed model for continous data 3.7.3 Mixed model for discrete data 3.7.4 Alternative approach to dealing with clustered data "],["longitudinal-study-and-time-series-data-1.html", "3.8 Longitudinal study and time series data", " 3.8 Longitudinal study and time series data "],["controlled-trials-1.html", "3.9 Controlled trials", " 3.9 Controlled trials "],["risk-analysis.html", "Chapter 4 Risk analysis", " Chapter 4 Risk analysis "],["spatial-epidemiology.html", "Chapter 5 Spatial Epidemiology ", " Chapter 5 Spatial Epidemiology "],["introduction-about-qgis.html", "5.1 Introduction about QGIS", " 5.1 Introduction about QGIS "],["spatial-epidemiology-1.html", "5.2 Spatial Epidemiology", " 5.2 Spatial Epidemiology "],["estimating-true-prevalence-of-a-disease.html", "Chapter 6 Estimating true prevalence of a disease ", " Chapter 6 Estimating true prevalence of a disease "],["se-and-sp-estimations-using-logistic-regression.html", "6.1 Se and Sp estimations using logistic regression", " 6.1 Se and Sp estimations using logistic regression "],["estimate-true-prevalence-without-a-gold-standard.html", "6.2 Estimate true prevalence without a gold standard", " 6.2 Estimate true prevalence without a gold standard "],["infectious-diseases-modelling.html", "Chapter 7 Infectious Diseases Modelling ", " Chapter 7 Infectious Diseases Modelling "],["introduction-about-compartmental-model.html", "7.1 Introduction about compartmental model", " 7.1 Introduction about compartmental model "],["introduction-about-system-dynamic-modelling.html", "7.2 Introduction about system dynamic modelling", " 7.2 Introduction about system dynamic modelling "],["other-models.html", "Chapter 8 Other models ", " Chapter 8 Other models "],["structure-equation-modelling.html", "8.1 Structure Equation Modelling", " 8.1 Structure Equation Modelling "],["network-analysis.html", "8.2 Network analysis", " 8.2 Network analysis "],["meta-analysis.html", "8.3 Meta analysis", " 8.3 Meta analysis "],["other-tools.html", "Chapter 9 Other tools ", " Chapter 9 Other tools "],["digital-data-collection.html", "9.1 Digital data collection", " 9.1 Digital data collection "],["powerbi.html", "9.2 PowerBI", " 9.2 PowerBI "]]
