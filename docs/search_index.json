[["index.html", "Applied Epidemiology in One Health Research Chapter 1 Basic Epidemiology", " Applied Epidemiology in One Health Research Author: Nguyn Thanh Lng Date updated: 2021-02-02 Chapter 1 Basic Epidemiology OBJECTIVES After reading this chapter, reader should be able to: Construct a logical causal diagram based on your area of research interest as an aid to guiding your study design and analyses Select the appropriate sampling strategy for a particular situation, taking into account the requirements, advantages and disadvantages of each method. Identify the different types of selection bias and assess whether or not a particular study is likely to suffer from excess selection bias. Explain the different ways of measuring disease frequency and differentiate among counts, proportions, odds and rates. Calculate and interpret the following measures of association: risk ratio, odds ratio, incidence rate ratio, risk difference (attributable risk), attributable fraction (exposed), population attributable risk, attributable fraction (population). Understand the strength and weaknesses of various study designs for the identification and evaluation of causal factors. "],["about-the-book.html", "1.1 About the book", " 1.1 About the book Our rationale for doing research is to identify potentially causal associations between exposures and outcomes (the center of the diagram). In many cases, the exposures are risk factors and the outcome is a disease of interest. However, this is not the only scenario; for example, our outcome of interest might be a measure of productivity or food safety and the exposures might include certain diseases. Figure 1.1. Key components of epidemiologic research Ultimately, we aim to make causal inferences (bottom right of diagram) and Chapter 1 discusses some important concepts of causation as they relate to epidemiologic research. Any study starts with an overall study design and the main observational study types are discussed in Chapters 1.6. In any study, it is important to identify the target population and obtain a study group from it in a manner that does not lead to selection bias. Sampling and selection bias is discussed in Chapter 1.3. Once we have identified our study subjects, it is necessary to obtain data on exposure variables, extraneous variables and the outcome in a manner that does not lead to information bias (Chapter 1.7). Two important tools that are used in that process are questionnaires (Chapter 1.4) and diagnostic and screening tests (Chapter 1.4). In order to start the process of establishing an association between exposure and outcome, we need to settle on a measure of disease frequency (Chapter 1.3) and select a measure of association (Chapter 1.5) that fits the context. In many cases, the study design will determi4e the measures that are appropriate. Confounding bias is a major concern in observational studies, and the identification of factors that should be controlled as confounders is featured in Chapter 1.7. Basic data analysis will be presented in Chapter 2. You will learn essential skills to conduct data analysis with different programming languages such as R and Python or statistical software like STATA. You will also introduced on how to communicate your works with other scientists and readers using Rmarkdown (Chapter 2.7) and ShinyApp (Chapter 2.8). Chapter 2.9 will provides you a road map for investigators starting into the analysis of an epidemiologic dataset. With our data in hand, we are now able to begin to model relationships with the intent of estimating causal effects of exposure (Chapter 3). Individual chapters are dedicated to the analyses appropriate for outcomes that are continuous (Chapter 3.2), dichotomous (Chapter 3.3), nominal/ordinal (Chapter 3.4), count (Chapter 3.5) and time-to-event data (Chapter 3.6). Chapter 3.1 presents some general guide lines on model-building techniques that are applicable to all types of model. In one health epidemiologic research, we often encounter clustered or correlated data and these present major challenges in their analyses. Chapter 3.7.1 introduces these while Chapters 3.7.2 and Chapters 3.7.3 focus on mixed (random effects) models for continuous and discrete outcomes. Chapters 3.7.4 presents some alternative methods of analysis for dealing with clustered data. Other important tools in epidemiology, including Risk analysis (Chapter 4), Spatial epidemiology (Chapter 5), Estimating true prevalence of a diseases (Chapter 6) and Infections diseases modelling (Chapter 7) will also be discussed Other models and tools will be mentioned in Chapter 8. Structured reviews and assessments of the literature in the form of meta-analyses are becoming increasingly important and are introduced in Chapter 8.3. In Chapter 9, we will introduce about digital data collection tool (Chapter 9.1) and visualization tools (Chapter 9.2) "],["causual-concept.html", "1.2 Causual concept", " 1.2 Causual concept 1.2.1 Experimental versus observational evidence Experimental evidence A major goal for epidemiologic research is to identify factors that can be manipulated to maximise health or prevent disease. In other words, we need to identify causes of health and disease. For our purposes, a cause is any factor that produces a change in the severity or frequency of the outcome. In searching for causes, we stress the holistic approach to health. The term holistic might suggest that we try to identify and measure every suspected causal factor for the outcome of interest. Yet, quite clearly, we cannot consider every possible factor in a single study. Rather, we place limits on the portion of the real world we study and, within this, we constrain the list of factors we identify for investigation. Traditionally, the gold standard approach to identifying causal factors is to perform an experiment. In the ideal experiment, we randomise some animals (or human) to receive the factor and some to receive nothing, a placebo, or a standard intervention (treatment). In this context, exposure X is a proven cause of outcome Y, if in an ideal experiment X is changed and, as a result, the value or state of Y also changes. In this example, X explicitly precedes Y temporally and an variables (known and unknown) that do not intervene between X and Y are made independent of X through the process of randomisation (this means that extraneous variables do not confound or bias the results we attribute to the exposure X). Factors that are positioned temporally or causally between X and Y are not measured and are of no concern with respect to answering the causal objective of the trial. The measure of causation in this ideal trial is called the causal effect coefficient and indicates the difference in the outcome between the treated and non-treated groups. For example, if the risk of the outcome in the group receiving the treatment is denoted R1 and the risk in the group not receiving the treatment is R0, then we might choose to measure the effect of treatment using either an absolute measure (ie risk difference - RD) or a relative measure (ie risk ratio - RR). If this difference is greater than what could be attributed to chance, then we would say that we have proved that the factor is a cause of the outcome event. A key point is that all causal-effect statements are based on contrasts of treatment levels; the outcome in the treated group cannot be interpreted without knowing the outcome in the untreated group. A second key feature is exchangeability; that is the same outcome would be observed (except for sampling error) if the assignments of treatment to study subjects had been reversed (ie if the treated group had been assigned to be untreated). Randomisation provides the probabilistic basis for the validity of this assumption. Observational evidence ln observational studies, we estimate the difference in values of Y between units that happen to have different values of X. We do not control whether a subject is, or is not, exposed. Variables related to both X and Y and which do not intervene between X and Y, can be controlled analytically or through matching or restricted sampling. The appropriate measure of association (eg a risk ratio or regression coefficient) reflecting the difference in the value of Y between the exposed and non-exposed groups can be used to obtain a reasonable estimate of the causal-effect coefficient that would be obtained in the ideal experiment. The major differences between observational studies and field experiments lie in the ability to prevent selection, misclassification and confounding bias, and dealing with the impact of unknown or unmeasured factors. Thus, by themselves, observational studies produce measures of association but cannot prove causation. Nonetheless, in the ideal observational study, with total control of bias, the measure of association will estimate the causal-effect coefficient. 1.2.2 Model of causation Given our belief in multiple causes of an effect and multiple effects of a specific cause, epidemiologists have sought to develop conceptual models of causation. Usually, however, the actual causal model is unknown and the statistical measures of association we use reflect, but do not explain, the number of ways in which the exposure might cause disease. Furthermore, although our main interest in a particular study might focus on one exposure factor, we need to take into account the effects of other causes of the outcome that are related to the exposure. The two major conceptual models are the component-cause and the causal-web models of causation. Component-cause model The component-cause model is based on the concepts of necessary and sufficient causes. A necessary cause is one without which the disease cannot occur (ie the factor will always be present if the disease occurs). In other word, a component cause is one of a number of factors that, in combination, constitute a sufficient cause. The factors might be present concomitantly or they might follow one another in a chain of events. In tum, when there are a number of chains with one or more factors in common, we can conceptualise the web of causal chains (ie a causal web). This concept will be explained further under the causal-web model. One of the benefits of thinking about causation in this manner is that it helps us understand how the prevalence of a cofactor can impact on the strength of association between the exposure factor and the outcome of interest. Causal-web model A second way of conceptualising how multiple factors can combine to cause disease is through a causal web consisting of indirect and direct causes. This concept is based on a series of interconnected causal chains or web structures; it takes the factors portrayed in the sufficient-cause approach and links them temporally. For a direct cause, there must be no known intervening variable between that factor and the disease (diagrammatically, the exposure is adjacent to the outcome). Direct causes are often the proximal causes emphasised in therapy, such as specific microorganisms or toxins. In contrast, an indirect cause is one in which the effects of the exposure on the outcome are mediated through one or more intervening variables. It is important to recognise that, in terms of disease control, direct causes are no more valuable than indirect causes. With a number of possible causal variables, the cause-and-effect relationships are best shown in a causal diagram (also called directed acyclic graphs, or modified path models). To construct a causal diagram, we begin by imposing a plausible biological causal structure on the set of variables we plan to investigate and translate this structure into graphical form that explains our hypothesised and known relationships among the variables. The causal-ordering assumption is usually based on known time-sequence and/or plausibility considerations. For example, it might be known that one variable precedes another temporally, or current knowledge and/or common sense might suggest that it is possible for one factor to cause another but not vice-versa. Association between Maternal Obesity and Child Respiratory Outcomes Red arrows: open biasing paths; Green arrows: open causal paths; Pink oval: ancestor of exposure; Blue ovals: ancestor of outcome. 1.2.3 Causal criteria A list of criteria for making valid causal inferences includes: time sequence, strength of association, dose-response, plausibility, consistency, specificity, analogy and experimental evidence. Today, we might add evidence from meta-analysis to this list. Study design and statistical issues If the differences observed in a well-designed study have P-values above 0.4, this would not provide any support for a causal relationship. However, beyond extremes in large P-values, statistical significance should not play a pivotal role in assessing causal relationships. Time sequence While a cause must precede its effect, demonstrating this fact provides only weak support for causation. Further, the same factor could occur after disease in some individuals and this would not disprove causation except in these specific instances. Many times it is not clear which came first; for example, did the viral infection precede or follow respiratory disease? This becomes a greater problem when we must use surrogate measures of exposure (eg antibody titre to indicate recent exposure). Nonetheless, we would like to be able to demonstrate that an exposure preceded the effect or at least develop a rational argument for believing that it did - sometimes these arguments are based largely on plausibility rather than on demonstrable facts Strength of association This is usually measured by ratio measures such as risk ratio or odds ratio but could also be measured by risk or rate differences. The belief in larger (stronger) associations being causal appears to relate to how likely it is that unknown or residual confounding might have produced this effect. When studying diseases with very high frequency, risk ratio measures of association will tend to be weaker than with less common diseases. Dose response relationship If we had a continuous, or ordinal, exposure variable and the risk of disease increased directly with the level of exposure, then this evidence supports causation as it tends to reduce the likelihood of confounding and is consistent with biological expectations. However, in some instances, there might be a cutpoint of exposure such that nothing happens until a threshold exposure is reached and there is no further increase in frequency at higher levels of exposure. Coherence or plausibility Coherence requires that the observed association is explicable in terms of what we know about disease mechanisms. However, our knowledge is a dynamic state and ranges all the way from the observed association being assessed as reasonable (without any biological supporting evidence) to requiring that all the facts be known (a virtually nonexistent state currently). Postulating a biological mechanism to explain an association after the fact is deemed to be insufficient for causal inferences unless there is some additional evidence supporting the existence of that mechanism Consistency If the same association is found in different studies by different workers, this gives support to causality. Lack of consistency doesnt mean that we should ignore the results of the first study on a subject, but we should temper our interpretation of the results until they are repeated. This would prevent a lot of false positive scares in both human and veterinary medicine. Specificity of association It used to be thought that, if a factor was associated with only one disease, it was more likely causal than a factor that was associated with numerous disease outcomes. Analogy This is not a very important criterion for assessing causation, although there are examples of its being used to good purpose. This approach tends to be used to infer relationships in cases of human diseases based on experimental results in other animal species. Experimental evidence This criterion perhaps relates partly to biological plausibility and partly to the additional control that is exerted in weIl-designed experiments. We tend to place more importance on experimental evidence if the same target species is used and the routes of challenge, or nature of the treatment are in line with what one might expect under field conditions. Experimental evidence from other species in more contrived settings is given less weight in our assessment of causation. Indeed, the experimental approach is just another way to test the hypothesis, so this is not really a distinct criterion for causation in its own right. "],["sampling.html", "1.3 Sampling", " 1.3 Sampling Chapter 2 v√† Chapter 12 "],["measures-of-diseases-frequency.html", "1.4 Measures of diseases frequency", " 1.4 Measures of diseases frequency Chapter 4 "],["measures-of-association.html", "1.5 Measures of association", " 1.5 Measures of association Chapter 6 "],["introduction-about-different-studies-design.html", "1.6 Introduction about different studies design", " 1.6 Introduction about different studies design 1.6.1 Introduction about observational studies Chapter 7 1.6.2 Cohort studies Chapter 8 1.6.3 Case-control studies Chapter 9 1.6.4 Hybrid study designs Chapter 10 1.6.5 Longitudinal study and time series data To be searched 1.6.6 Controlled trials Chapter 11 "],["confounder-bias.html", "1.7 Confounder bias", " 1.7 Confounder bias Chapter 13 "],["basic-data-analysis.html", "Chapter 2 Basic Data Analysis", " Chapter 2 Basic Data Analysis OBJECTIVES After reading this chapter, reader should be able to: Understand the work flow and the concept of tidy data Install and set up working environment for different languages and software, including R, Python and STATA in Rstudio IDE Understand the principles and steps of data manipulation and implement in R, Python and STATA. Perform basic descriptive statistics with R, Python and STATA such as mean, sd, frequency, statistical tests (Chi-squared, t-test, etc.) Visualize data with ggplot2 package in R and matplotlib library in Python Communicate your result with Rmarkdown or deploying a simple project with ShinyApp "],["introduction-about-work-flow.html", "2.1 Introduction about work flow", " 2.1 Introduction about work flow The picture below described about a normal working process of data analysis. First you must import your data. This typically means that you take data stored in a file, database, or web application programming interface (API), and load it into the software. In this course, we will work with .xlsx and .csv file, which are the most common type of storaging data. Source: R for Data Science book The next steps are Tidying and Transforming your data. Combining 2 steps, we have the so-called Data Wrangling. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Once you have tidy data, a common first step is to transform it. Transformation includes narrowing in on observations of interest, creating new variables that are functions of existing variables, and calculating a set of summary statistics (like counts or means). Visualization is a fundamentally human activity. A good visualization will show you things that you did not expect, or raise new questions about the data. A good visualization might also hint that youre asking the wrong question, or you need to collect different data. Visualizations can surprise you, but dont scale particularly well because they require a human to interpret them. Models are complementary tools to visualization. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you. The last step of data science is Communication, an absolutely critical part of any data analysis project. It doesnt matter how well your models and visualization have led you to understand the data unless you can also communicate your results to others. "],["software-installation.html", "2.2 Software installation", " 2.2 Software installation In this course, I will introduce you about the three most powerful statistical softwares including R, Python and STATA, their usage and their advantages as well as disadvantages. While R and Python is more a programming language, which can be a hurdle for some people, STATA is more user-friendly and balance between using Graphical User Interface (GUI) and saving script (do file). In short, youll need to install the following programs in order: R: A statistical programming language used to wrangle, analyze, and visualize data (mac, windows) RStudio: An interface for writing and running R code, which is a primary language for the quarter (link). You can download a developing version here, which will provide most up-to-date new features Python: Another programming language, be preferred in machine learning, deep learning, AI. In this tutorial, we will install Python indirectly through package reticulate in R. More details will be presented later Git: (Optional) A set of command-line tools for tracking changes to a project. This is likely already installed on Macs. The Windows download will come with Git Bash, a simple interface for executing Git commands (link) STATA: This is paid software. You have to buy it before use or you can download a trial version here The following sections have additional information about the purpose of each component, how to install it, and alternative configurations. 2.2.1 R R is a popular data science language used to download, analyze, and visualize data. You can download it at the appropriate link for your operating system (mac, windows). At the link, click the appropriate download link and follow instructions: To install package in R, type the following code to the console. For instance, you want to install tidyverse package install.packages(&quot;tidyverse&quot;) To call tidyverse package library(tidyverse) 2.2.2 Rstudio The primary programming language we will use throughout the course is R. Its a very powerful statistical programming language that is built to work well with large and diverse datasets. While you are able to execute R scripts without an interface, the RStudio interface provides a wonderful way to engage with the R language. Importantly, you cannot use the RStudio interface until you have installed R. To download the RStudio program, select the installer for your operating system at this link. Make sure to scroll down to download a free version of RStudio: 2.2.3 Python Python is a very popular all-purpose programming language that is making a major impact in the data-science arena. Some data scientists, and even some organizations, believe they have to pick between R or Python. However, this turns out to be a false choice. In fact, results from a surveys showed that many data science teams today are bilingual, leveraging both R and Python in their work. And while both languages have unique strengths, these teams frequently struggle to use them together. Rstudio provides a prefect environment for data scientist through a package reticulate. Rstudio can be a potentially platform for data scientist who are using solely R or Python, or using both. 2.2.4 Setting environment for R and Python Firstly, you have to install reticulate package: install.packages(&quot;reticulate&quot;) Secondly, install the miniconda environment. I suggest you should install miniconda in your local disk, not your OS disk (i.e Disk C) reticulate::install_miniconda(&quot;D:/Python&quot;) reticulate::use_condaenv(&quot;D:/Python/envs/r-reticulate&quot;) Thirdly, try to install Python package. There are two ways to do it, Reticulate style and Python style Reticulate style library(reticulate) py_install(&quot;pandas&quot;) py_install(&quot;matplotlib&quot;) Python style Set Python PATH through Terminal. You copy this code and paste into Terminal console in your Rstudio: setx PATH \"%PATH%;D:\\Python\\envs\\r-reticulate\\Scripts\" Then try to run this code to install pandas package: python -m pip install pandas 2.2.5 STATA This is paid software. You have to buy it before use or you can download a trial version here If you want to use STATA in Rstudio, you can do it through statamarkdown package. Detail will be dicussed further in this document. install.packages(&quot;Statamarkdown&quot;) library(Statamarkdown) stataexe &lt;- &quot;C:/Program Files/Stata16/StataSE-64.exe&quot; knitr::opts_chunk$set(engine.path=list(stata=stataexe)) "],["create-working-environment.html", "2.3 Create working environment", " 2.3 Create working environment Git is a version control system that provides a set of commands that allow you to manage changes to a project (much more on this in module-3). For now, youll need to download and install the software. Note, if you are using a Windows machine, this will install a program called Git Bash, which provides a text-based interface for executing commands on your computer. To setup Git in your Rstudio, you can visit this link. For alternative/additional Windows command-line tools, see below: Git Bash Because well primarily use the command line for implementing version control (i.e., keeping track of changes to our code), we can use a command-line tool that ships with the version control software, Git. When you download the Git software on Windows, the Git Bash user-interface will be installed. You can then navigate to Git Bash from your Desktop / Start Menu, and you will be able to use the appropriate syntax to keep track of code changes. Windows Bash With the release of Windows 10, Windows began providing command line (bash) support. If you already have Windows 10, here are a few instructions for installing bash capabilities. This requires that you switch to 64 bit windows, and follow the instructions above. While this will provide you with direct bash capabilities, you may run into challenges along the way (I have not tested these instructions). Note, you will still need to install Git in addition to Windows Bash. Powershell (Windows Management Framework) If you want to explore more robust command-line alternatives for Windows, the Windows Management Framework (including a program called Powershell) seems to be a preferred standard. Powershell will provide a simple text-based interface for inputing commands. Note, you will still need to install Git in addition to Powershell. "],["data-manipulation.html", "2.4 Data manipulation", " 2.4 Data manipulation 2.4.1 Import file There are several ways to import data, but the most preferred is from comma-separated values file (.csv). Other data types format you may normally see in your work are from text file (.txt), Excel spread sheets (.xlsx), STATA file (.dta), SPSS file (.sav), JavaScript Object Notation (.json), shapefile (.shp). In this tutorial, I will only concentrate on importing data from CSV file because of its ubiquitous in data analysis. We will use calf data set, you can download it here. Dataset description These data come from a retrospective analysis of the medical records from all diarrheic calves which were presented to Atlantic Veterinary College, PEl, Canada between 1989 and 1993. There are 254 observations (records) and 14 variables in the dataset (calf): Variables Description Codes/Units case hospital case number age age at admission days breed breed coded 1-9 sex sex 0 = female 1 = male attd attitude of calf 0 = bright, alert 1 = depressed 2 = unresponsive, comatose dehy % dehydration eye uveitis/hypopyon clinically evident 0/1 jnts swollen joints clinically evident number of joints affected post posture of caIf 0 = standing 1 = sternal 2 = lateral pulse pulse rate beats per minute resp respiratory rate breaths per minute temp rectal temperature 0C umb swollen umbilicus clinically evident 0/1 sepsis sepsis (outcome) 0/1 R To import data in R, you need readr package installed in your computer and use function read_csv to import file to the working environment. library(readr) raw_df &lt;- read_csv(&quot;D:/Book Writing/1. Applied Epidemiology in One Health/data/calf.csv&quot;) glimpse(raw_df) ## Rows: 254 ## Columns: 14 ## $ case &lt;dbl&gt; 1670, 8124, 6954, 2737, 5341, 6749, 3234, 2325, 2925, 7108, ... ## $ age &lt;dbl&gt; 5, 3, 2, 3, 3, 10, 4, 14, 4, 8, 19, 7, 6, 2, 18, 7, 7, 8, 12... ## $ breed &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ## $ sex &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, ... ## $ attd &lt;dbl&gt; 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 2, 0, 1, ... ## $ dehy &lt;dbl&gt; 12.0, 13.5, NA, 5.0, 0.0, 5.5, 7.0, 7.0, 5.0, 10.0, 8.0, 4.0... ## $ eye &lt;dbl&gt; NA, 0, 1, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ jnts &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ## $ post &lt;dbl&gt; 2, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 2, 0, 1, ... ## $ pulse &lt;dbl&gt; NA, 130, NA, 132, 128, 160, 180, 100, 150, 120, 140, 126, 10... ## $ resp &lt;dbl&gt; NA, 120, NA, 40, 48, 48, 84, 20, 52, 68, 32, 20, 20, 16, 40,... ## $ temp &lt;dbl&gt; 37.6, 39.2, NA, 38.6, 38.6, 39.6, 39.0, 35.0, 39.5, 37.8, 39... ## $ umb &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ... ## $ sepsis &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, ... Python First, you have to register to use Python in R via package reticulate reticulate::use_condaenv(&quot;D:/Python/envs/r-reticulate&quot;) import os as os os.environ[&#39;QT_QPA_PLATFORM_PLUGIN_PATH&#39;] = &quot;D:/Python/envs/r-reticulate/Library/plugins/platforms&quot; Now, you can import CSV file by Python using library pandas with function read_csv import pandas as pd raw_df = pd.read_csv(&quot;D:/Book Writing/1. Applied Epidemiology in One Health/data/calf.csv&quot;) raw_df.head() ## case age breed sex attd dehy ... post pulse resp temp umb sepsis ## 0 1670 5.0 1 1.0 2.0 12.0 ... 2.0 NaN NaN 37.6 0.0 0 ## 1 8124 3.0 2 0.0 1.0 13.5 ... 0.0 130.0 120.0 39.2 0.0 0 ## 2 6954 2.0 3 1.0 2.0 NaN ... 2.0 NaN NaN NaN 1.0 1 ## 3 2737 3.0 4 1.0 1.0 5.0 ... 0.0 132.0 40.0 38.6 0.0 0 ## 4 5341 3.0 5 0.0 1.0 0.0 ... 1.0 128.0 48.0 38.6 1.0 1 ## ## [5 rows x 14 columns] STATA First, setup STATA code chunk in Rmarkdown library(Statamarkdown) stataexe &lt;- &quot;C:/Program Files/Stata16/StataSE-64.exe&quot; knitr::opts_chunk$set(engine.path=list(stata=stataexe)) Then, use function import delimited, specify your path to the data import delimited &quot;D:\\Book Writing\\1. Applied Epidemiology in One Health\\data\\calf.csv&quot; describe . import delimited &quot;D:\\Book Writing\\1. Applied Epidemiology in One Health\\da&gt; calf.csv&quot; (14 vars, 254 obs) . describe Contains data obs: 254 vars: 14 ------------------------------------------------------------------------------- storage display value variable name type format label variable label ------------------------------------------------------------------------------- case int %8.0g age byte %8.0g breed byte %8.0g sex byte %8.0g attd byte %8.0g dehy float %9.0g eye byte %8.0g jnts byte %8.0g post byte %8.0g pulse int %8.0g resp int %8.0g temp float %9.0g umb byte %8.0g sepsis byte %8.0g ------------------------------------------------------------------------------- Sorted by: Note: Dataset has changed since last saved. 2.4.2 Labelling data 2.4.2.1 R To label for value of variables in dataset, you can use function mutate to create new varibales (will present in details later) combining with factor function. Remember that you can only do that for categorical variables, in R they is storaged as factor data. Noted that the operation |&gt; means forwarding. It is R native piping, similar to %&gt;% if you know magrittr package lb_df &lt;- raw_df |&gt; mutate(breed = factor(breed, level = c(1:9), labels = c(&quot;Hereford&quot;, &quot;Holstein&quot;,&quot;Angus&quot;, &quot;Shorthorn&quot;, &quot;Charolais&quot;, &quot;Simmental&quot;, &quot;Limousin&quot;, &quot;Crossbreed&quot;, &quot;Other&quot;)), sex = factor(sex, level = c(0:1), labels = c(&quot;Female&quot;,&quot;Male&quot;)), attd = factor(attd, level = c(0:2), labels = c(&quot;bright, alert&quot;,&quot;depressed&quot;, &quot;unresponsive, comatose&quot;)), eye = factor(eye, level = c(0:1), labels = c(&quot;No&quot;,&quot;Yes&quot;)), post = factor(post, level = c(0:2), labels = c(&quot;standing&quot;,&quot;sternal&quot;, &quot;lateral&quot;)), umb = factor(umb, level = c(0:1), labels = c(&quot;No&quot;,&quot;Yes&quot;)), sepsis = factor(sepsis, level = c(0:1), labels = c(&quot;No&quot;,&quot;Yes&quot;))) To label for variables in dataset, you can use function var_labels in package sjlabelled. library(magrittr) lb_df %&lt;&gt;% sjlabelled::var_labels( case = &quot;hospital case number&quot;, age = &quot;age at admission&quot;, breed = &quot;breed&quot;, sex = &quot;sex&quot;, attd = &quot;attitude of calf&quot;, dehy = &quot;% dehydration&quot;, eye = &quot;uveitis/hypopyon clinically evident&quot;, jnts = &quot;swollen joints clinically evident&quot;, post = &quot;posture of caIf&quot;, pulse = &quot;pulse rate&quot;, resp = &quot;respiratory rate&quot;, temp = &quot;rectal temperature&quot;, umb = &quot;swollen umbilicus clinically evident&quot;, sepsis = &quot;sepsis&quot;) You can view the result through function view(lb_df), it will open the window like this: 2.4.2.2 Python Python doesnt support to handle with labelled data, or complicated to do. One simple solution is we do it in R and then pass to Pandas data frame by calling function r. lb_df = r.lb_df If you still want to try, an alternative is creating a dictionary that contain all value of variables. Then using mutate function from siuba library which is similar to dplyr library in R Tidyverse to mutate new variable breed_dict = {1 : &quot;Hereford&quot;, 2 : &quot;Holstein&quot;, 3 : &quot;Angus&quot;, 4 : &quot;Shorthorn&quot;, \\ 5 : &quot;Charolais&quot;, 6: &quot;Simmental&quot;, 7 : &quot;Limousin&quot;, 8: &quot;Crossbreed&quot;, 9 : &quot;Other&quot;} sex_dict = {0 : &quot;Female&quot;, 1 : &quot;Male&quot;} attd_dict = {0 : &quot;bright, alert&quot;, 1: &quot;depressed&quot;, 2: &quot;unresponsive, comatose&quot;} eye_dict = {0 : &quot;No&quot;, 1 : &quot;Yes&quot;} post_dict = {0 : &quot;standing&quot;, 1: &quot;sternal&quot;, 2 : &quot;lateral&quot;} umb_dict = {0 : &quot;No&quot;, 1 : &quot;Yes&quot;} sepsis_dict = {0 : &quot;No&quot;, 1 : &quot;Yes&quot;} from siuba import * lb_df1 = raw_df &gt;&gt; mutate( sex = _.apply(lambda x: sex_dict.get(x[&quot;sex&quot;], &quot;NaN&quot;), axis = 1), breed = _.apply(lambda x: breed_dict.get(x[&quot;breed&quot;], &quot;NaN&quot;), axis = 1), attd = _.apply(lambda x: attd_dict.get(x[&quot;attd&quot;], &quot;NaN&quot;), axis = 1), eye = _.apply(lambda x: eye_dict.get(x[&quot;eye&quot;], &quot;NaN&quot;), axis = 1), post = _.apply(lambda x: post_dict.get(x[&quot;post&quot;], &quot;NaN&quot;), axis = 1), umb = _.apply(lambda x: umb_dict.get(x[&quot;umb&quot;], &quot;NaN&quot;), axis = 1), sepsis = _.apply(lambda x: sepsis_dict.get(x[&quot;sepsis&quot;], &quot;NaN&quot;), axis = 1)) You can do exactly same version with assign function from pandas library. The principle is same because siuba was developed inherited from pandas, but lighter and support for R users who are familiar with tidyverse ecosystem. lb_df2 = raw_df.assign( sex = _.apply(lambda x: sex_dict.get(x[&quot;sex&quot;], &quot;NaN&quot;), axis = 1), breed = _.apply(lambda x: breed_dict.get(x[&quot;breed&quot;], &quot;NaN&quot;), axis = 1), attd = _.apply(lambda x: attd_dict.get(x[&quot;attd&quot;], &quot;NaN&quot;), axis = 1), eye = _.apply(lambda x: eye_dict.get(x[&quot;eye&quot;], &quot;NaN&quot;), axis = 1), post = _.apply(lambda x: post_dict.get(x[&quot;post&quot;], &quot;NaN&quot;), axis = 1), umb = _.apply(lambda x: umb_dict.get(x[&quot;umb&quot;], &quot;NaN&quot;), axis = 1), sepsis = _.apply(lambda x: sepsis_dict.get(x[&quot;sepsis&quot;], &quot;NaN&quot;), axis = 1)) glimpse(lb_df) Shape: (254, 14) case float64 0 (0%) NAs : 1670.0, 8124.0, 6954.0, 2737.0, 5341.0, 6749.0, 3234.0, 2325.0, 2925.0, 7108.0 age float64 1 (0%) NAs : 5.0, 3.0, 2.0, 3.0, 3.0, 10.0, 4.0, 14.0, 4.0, 8.0 breed category 0 (0%) NAs : Hereford, Holstein, Angus, Shorthorn, Charolais, Simmental, Limousin, Crossbreed sex category 2 (1%) NAs : Male, Female, Male, Male, Female, Female, Female, Female, Female, Female attd category 6 (2%) NAs : unresponsive, comatose, depressed, unresponsive, comatose, depressed, depressed, dehy float64 14 (6%) NAs : 12.0, 13.5, nan, 5.0, 0.0, 5.5, 7.0, 7.0, 5.0, 10.0 eye category 18 (7%) NAs : nan, No, Yes, No, No, No, No, No, No, No jnts float64 11 (4%) NAs : 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0 post category 6 (2%) NAs : lateral, standing, lateral, standing, sternal, standing, lateral, standing, stan pulse float64 9 (4%) NAs : nan, 130.0, nan, 132.0, 128.0, 160.0, 180.0, 100.0, 150.0, 120.0 resp float64 19 (7%) NAs : nan, 120.0, nan, 40.0, 48.0, 48.0, 84.0, 20.0, 52.0, 68.0 temp float64 7 (3%) NAs : 37.6, 39.2, nan, 38.6, 38.6, 39.6, 39.0, 35.0, 39.5, 37.8 umb category 11 (4%) NAs : No, No, Yes, No, Yes, No, Yes, No, No, No sepsis category 0 (0%) NAs : No, No, Yes, No, Yes, No, Yes, No, No, No 2.4.2.3 STATA label var case &quot;hospital case number&quot; label var age &quot;age at admission&quot; label var breed &quot;breed&quot; label def breed 1 &quot;Hereford&quot; 2 &quot;Holstein&quot; 3 &quot;Angus&quot; 4 &quot;Shorthorn&quot; 5 &quot;Charolais&quot; 6 &quot;Simmental&quot; 7 &quot;Limousin&quot; 8 &quot;Crossbreed&quot; 9 &quot;Other&quot; label val breed breed label var sex &quot;sex&quot; label def sex 0 &quot;Female&quot; 1 &quot;Male&quot; label val sex sex label var attd &quot;attitude of calf&quot; label def attd 0 &quot;bright, alert&quot; 1 &quot;depressed&quot; 2 &quot;unresponsive, comatose&quot; label val attd attd label var dehy &quot;% dehydration&quot; label var eye &quot;uveitis/hypopyon clinically evident&quot; label def eye 0 &quot;No&quot; 1 &quot;Yes&quot; label val eye eye label var jnts &quot;swollen joints clinically evident&quot; label var post &quot;posture of caIf&quot; label def post 0 &quot;standing&quot; 1 &quot;sternal&quot; 2 &quot;lateral&quot; label val post post label var pulse &quot;pulse rate&quot; label var resp &quot;respiratory rate&quot; label var temp &quot;rectal temperature&quot; label var umb &quot;swollen umbilicus clinically evident&quot; label def umb 0 &quot;No&quot; 1 &quot;Yes&quot; label val umb umb label var sepsis &quot;sepsis&quot; label def sepsis 0 &quot;No&quot; 1 &quot;Yes&quot; label val sepsis sepsis 2.4.3 Data transformation 2.4.3.1 R In this section you are going to learn the five key dplyr functions that allow you to solve the vast majority of your data manipulation challenges: Pick observations by their values filter(). Reorder the rows arrange(). Pick variables by their names select(). Create new variables with functions of existing variables mutate(). Collapse many values down to a single summary summarise(). These can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation. All verbs work similarly: The first argument is a data frame. The subsequent arguments describe what to do with the data frame, using the variable names (without quotes). The result is a new data frame. Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Lets dive in and see how these verbs work. Create new variable 2.4.3.2 Python 2.4.3.3 STATA "],["descriptive-statistics.html", "2.5 Descriptive statistics", " 2.5 Descriptive statistics "],["data-visualization.html", "2.6 Data visualization", " 2.6 Data visualization "],["communication-with-rmarkdown.html", "2.7 Communication with Rmarkdown", " 2.7 Communication with Rmarkdown R Markdown is an open-source tool for producing reproducible reports in R. It enables you to keep all of your code, results, plots, and writing in one place. R Markdown is particularly useful when you are producing a document for an audience that is interested in the results from your analysis, but not your code. In general, you can not only use Rmarkdown for your R code, but you can integrate many others, such as Python and STATA like what we are doing in this tutorial book. R Markdown is powerful because it can be used for data analysis and data science, collaborating with others, and communicating results to decision makers. With R Markdown, you have the option to export your work to numerous formats including PDF, Microsoft Word, a slideshow, or an HTML document for use in a website. You can find a comprehensive guide from the book: R Markdown: The Definitive Guide 2.7.1 Start a document with Rmarkdown R Markdown is a free, open source tool that is installed like any other R package. Use the following command to install R Markdown: install.packages(&quot;rmarkdown&quot;) Now that R Markdown is installed, open a new R Markdown file in RStudio by navigating to File &gt; New File &gt; R Markdown. R Markdown files have the file extension .Rmd. When you open a new R Markdown file in RStudio, a pop-up window appears that prompts you to select output format to use for the document. The default output format is HTML. With HTML, you can easily view it in a web browser. 2.7.2 Rmarkdown document format Once youve selected the desired output format, an R Markdown document appears in your RStudio pane. But unlike an R script which is blank, this .Rmd document includes some formatting that might seem strange at first. Lets break it down. Weve highlighted six different sections of this R Markdown document to understand what is going on: YAML Header: Controls certain output settings that apply to the entire document. Code Chunk: Includes code to run, and code-related options. Body Text: For communicating results and findings to the targeted audience. Code to Generate a Table: Outputs a table with minimal formatting like you would see in the console. Section Header: Specified with ##. Code to Generate a Plot: Outputs a plot. Here, the code used to generate the plot will not be included because the parameter echo=FALSE is specified. This is a chunk option. Well cover chunk options soon! This document is ready to output as-is. Lets knit, or output, the document to see how these formatting specifications look in a rendered document. We do this in RStudio by clicking the knit button. Knitting the document generates an HTML document, because thats the output format weve specified. The shortcut to knit a document is Command + Shift + K on a Mac, or Ctrl + Shift + K on Linux and Windows. The k is short for knit! If you want to share your work to the internet, Rstudio provides a wonderful platform called Rpubs. To do this, after you knit your document, you click on the button Publish document on the top right, create an account on Rpubs and wait to see your results. 2.7.3 Control Behavior with Code Chunk Options One of the great things about R Markdown is that you have many options to control how each chunk of code is evaluated and presented. This allows you to build presentations and reports from the ground up  including code, plots, tables, and images  while only presenting the essential information to the intended audience. For example, you can include a plot of your results without showing the code used to generate it. Mastering code chunk options is essential to becoming a proficient R Markdown user. The best way to learn chunk options is to try them as you need them in your reports, so dont worry about memorizing all of this now. Here are the key chunk options to learn: echo = FALSE: Do not show code in the output, but run code and produce all outputs, plots, warnings and messages. The code chunk to generate a plot in the image below is an example of this. eval = FALSE: Show code, but do not evaluate it. fig.show = \"hide\": Hide plots. include = FALSE: Run code, but suppress all output. This is helpful for setup code. You can see an example of this in the top code chunk of the image below. message = FALSE: Prevent packages from printing messages when they load. This also suppress messages generated by functions. results = \"hide\": Hides printed output. warning = FALSE: Prevents packages and functions from displaying warnings. "],["shinyapp.html", "2.8 ShinyApp", " 2.8 ShinyApp https://blog.rstudio.com/2021/02/01/shiny-1-6-0/ https://www.youtube.com/watch?v=wSYt4IYIsdI https://yakdata.com/ultimate-guide-r-shiny-app-dashboards-2021/ https://github.com/nanxstats/awesome-shiny-extensions "],["a-structred-approach-to-data-analysis.html", "2.9 A structred approach to data analysis", " 2.9 A structred approach to data analysis "],["regression-models.html", "Chapter 3 Regression Models ", " Chapter 3 Regression Models "],["model-buidling-strategy.html", "3.1 Model buidling strategy", " 3.1 Model buidling strategy "],["linear-regression.html", "3.2 Linear regression", " 3.2 Linear regression "],["logistic-regression.html", "3.3 Logistic regression", " 3.3 Logistic regression "],["modelling-multinomial-data.html", "3.4 Modelling multinomial data", " 3.4 Modelling multinomial data "],["modelling-count-and-rate-data.html", "3.5 Modelling count and rate data", " 3.5 Modelling count and rate data "],["modelling-survival-data.html", "3.6 Modelling survival data", " 3.6 Modelling survival data "],["modelling-clustered-data.html", "3.7 Modelling clustered data", " 3.7 Modelling clustered data 3.7.1 Introduction to clustered data 3.7.2 Mixed model for continous data 3.7.3 Mixed model for discrete data 3.7.4 Alternative approach to dealing with clustered data "],["longitudinal-study-and-time-series-data-1.html", "3.8 Longitudinal study and time series data", " 3.8 Longitudinal study and time series data "],["controlled-trials-1.html", "3.9 Controlled trials", " 3.9 Controlled trials "],["risk-analysis.html", "Chapter 4 Risk Analysis", " Chapter 4 Risk Analysis "],["spatial-epidemiology.html", "Chapter 5 Spatial Epidemiology ", " Chapter 5 Spatial Epidemiology "],["introduction-about-qgis.html", "5.1 Introduction about QGIS", " 5.1 Introduction about QGIS "],["spatial-epidemiology-1.html", "5.2 Spatial Epidemiology", " 5.2 Spatial Epidemiology "],["estimating-true-prevalence-of-a-disease.html", "Chapter 6 Estimating true prevalence of a disease ", " Chapter 6 Estimating true prevalence of a disease "],["se-and-sp-estimations-using-logistic-regression.html", "6.1 Se and Sp estimations using logistic regression", " 6.1 Se and Sp estimations using logistic regression "],["estimate-true-prevalence-without-a-gold-standard.html", "6.2 Estimate true prevalence without a gold standard", " 6.2 Estimate true prevalence without a gold standard "],["infectious-diseases-modelling.html", "Chapter 7 Infectious Diseases Modelling ", " Chapter 7 Infectious Diseases Modelling "],["introduction-about-compartmental-model.html", "7.1 Introduction about compartmental model", " 7.1 Introduction about compartmental model "],["introduction-about-system-dynamic-modelling.html", "7.2 Introduction about system dynamic modelling", " 7.2 Introduction about system dynamic modelling "],["other-models.html", "Chapter 8 Other models ", " Chapter 8 Other models "],["structure-equation-modelling.html", "8.1 Structure Equation Modelling", " 8.1 Structure Equation Modelling "],["network-analysis.html", "8.2 Network analysis", " 8.2 Network analysis "],["meta-analysis.html", "8.3 Meta analysis", " 8.3 Meta analysis "],["other-tools.html", "Chapter 9 Other tools ", " Chapter 9 Other tools "],["digital-data-collection.html", "9.1 Digital data collection", " 9.1 Digital data collection "],["powerbi.html", "9.2 PowerBI", " 9.2 PowerBI "]]
